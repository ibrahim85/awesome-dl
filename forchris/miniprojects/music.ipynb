{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['savefig.dpi'] = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from grader import score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifing Music by Genre\n",
    "\n",
    "Music offers an extremely rich and interesting playing field. The objective of this miniproject is to develop models that are able to recognize the genre of a musical piece, first from pre-computed features and then working from the raw waveform. This is a typical example of a classification problem on time series data.\n",
    "\n",
    "Each piece has been classified to belong to one of the following genres:\n",
    "- electronic\n",
    "- folkcountry\n",
    "- jazz\n",
    "- raphiphop\n",
    "- rock\n",
    "\n",
    "The model will be assessed based on the accuracy score of your classifier.  There is a reference solution.  The reference solution has a score of 1. *(Note that this doesn't mean that the accuracy of the reference solution is 1)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "\n",
    "## Question 1: All Features Model\n",
    "Download a set of pre-computed features from Amazon S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!aws s3 sync s3://dataincubator-course/mldata/ . --exclude '*' --include 'df_train_anon.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains 549 pre-computed features for the training set. The last column contains the genre.\n",
    "\n",
    "Build a model to generate predictions from this feature set. Steps in the pipeline could include:\n",
    "\n",
    "- a normalization step (not all features have the same size or distribution)\n",
    "- a dimensionality reduction or feature selection step\n",
    "- ... any other transformer you may find relevant ...\n",
    "- an estimator\n",
    "- a label encoder inverse transform to return the genre as a string\n",
    "\n",
    "Use GridSearchCV to find the scikit learn estimator with the best cross-validated performance.\n",
    "\n",
    "*Hints:*\n",
    "- Use a dimensionality reduction technique (e.g. PCA) or a feature selection criteria when possible.\n",
    "- Use GridSearchCV to improve score.\n",
    "- Use a [LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) to generate an encoding for the labels.\n",
    "- The model needs to return the genre as a string. You'll need to create a wrapper class around scikit-learn estimators in order to do that.\n",
    "\n",
    "Submit a function that takes a list of records, each a list of the 549 features, and returns a list of genre predictions, one for each record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def all_features_est(records):\n",
    "    return ['blues' for r in records]\n",
    "\n",
    "score('music__all_features_model', all_features_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Raw Features Predictions\n",
    "\n",
    "For questions 2 and 3, you will need to extract features from raw audio.  Because this extraction can be rather time-consuming, you will not conduct the feature extraction of the test set in real time during the grading.\n",
    "\n",
    "Instead, you will download a set of test files.  After you have trained your model, you will run it on the test files, to make a prediction for each.  Then submit to the grader a dictionary of the form\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"fe_test_0001.mp3\": \"electronic\",\n",
    "  \"fe_test_0002.mp3\": \"rock\",\n",
    "  ...\n",
    "}\n",
    "```\n",
    "\n",
    "A sets of files for training and testing are available on Amazon S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training files\n",
    "!aws s3 sync s3://dataincubator-course/mldata/ . --exclude '*' \\\n",
    "    --include 'music_train.tar.gz' \\\n",
    "    --include 'music_train_labels.csv' \\\n",
    "    --include 'music_feature_extraction_test.tar.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All songs are sampled at 44100 Hz.\n",
    "\n",
    "The simplest features that can be extracted from a music time series are the [zero crossing rate](https://en.wikipedia.org/wiki/Zero-crossing_rate) and the [root mean square energy](https://en.wikipedia.org/wiki/Root_mean_square).\n",
    "\n",
    "1. Build a function or a transformer that calculates these two features starting from a raw file input.  In order to go from a music file of arbitrary length to a fixed set of features you will need to use a sliding window approach, which implies making the following choices:\n",
    "\n",
    " 1. what window size are you going to use?\n",
    " 2. what's the overlap between windows?\n",
    "\n",
    " Besides that, you will need to decide how you are going to summarize the values of such features for the whole song. Several strategies are possible:\n",
    " -  you could decide to describe their statistics over the whole song by using descriptors like mean, std and higher order moments\n",
    " -  you could decide to split the song in sections, calculate statistical descriptors for each section and then average them\n",
    " -  you could decide to look at the rate of change of features from one window to the next (deltas).\n",
    " -  you could use any combination of the above.\n",
    "\n",
    " Your goal is to build a transformer that will output a \"song fingerprint\" feature vector that is based on the 2 raw features mentioned above. This vector has to have the same size, regardless of the duration of the song clip it receives.\n",
    "\n",
    "2. Train an estimator that receives the features extracted by the transformer and predicts the genre of a song.  Your solution to Question 1 should be a good starting point.\n",
    "\n",
    "Use this pipeline to predict the genres for the 145 files in the `music_feature_extraction_test.tar.gz` set and submit your predictions as a dictionary in the `__init__.py` file.\n",
    "\n",
    "*Hints*\n",
    "- Extracting features from time series can be computationally intensive. Make sure you choose wisely which features to calculate.\n",
    "- You can use MRJob or PySpark to distribute the feature extraction part of your model and then train an estimator on the extracted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def raw_features_predictions():\n",
    "    return {(\"fe_test_%04d.mp3\" % i): 'blues' for i in xrange(1, 146)}\n",
    "\n",
    "score('music__raw_features_predictions', raw_features_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: All Features Predictions\n",
    "The approach of Question 2 can be generalized to any number and kind of features extracted from a sliding window. Use the [librosa library](https://github.com/librosa/librosa) to extract features that could better represent the genre content of a musical piece.\n",
    "You could use:\n",
    "- spectral features to capture the kind of instruments contained in the piece\n",
    "- MFCCs to capture the variations in frequencies along the piece\n",
    "- Temporal features like tempo and autocorrelation to capture the rhythmic information of the piece\n",
    "- features based on psychoacoustic scales that emphasize certain frequency bands.\n",
    "- any combination of the above\n",
    "\n",
    "As for question 1, you'll need to summarize the time series containing the features using some sort of aggregation. This could be as simple as statistical descriptors or more involved, your choice.\n",
    "\n",
    "As a general rule, build your model gradually. Choose few features that seem interesting, calculate the descriptors and generate predictions.\n",
    "\n",
    "Make sure you `GridSearchCV` the estimators to find the best combination of parameters.\n",
    "\n",
    "Use this pipeline to predict the genres for the 145 files in the `music_feature_extraction_test.tar.gz` set and submit your predictions as a static list in the `__init__.py` file.\n",
    "\n",
    "**Questions for Consideration:**\n",
    "1. Does your transformer make any assumption on the time duration of the music piece? If so how could that affect your predictions if you receive longer/shorter pieces?\n",
    "\n",
    "2. This model works very well on one of the classes. Which one? Why do you think that is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def all_features_predictions():\n",
    "    return {(\"fe_test_%04d.mp3\" % i): 'blues' for i in xrange(1, 146)}\n",
    "\n",
    "score('music__all_features_predictions', all_features_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2016 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
